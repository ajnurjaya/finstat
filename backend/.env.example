# === SERVER CONFIGURATION ===
HOST=0.0.0.0
PORT=8000

# === AI PROVIDER (Choose one) ===
# Option 1: Ollama (Local LLM - Recommended for on-premise)
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b-instruct-q5_K_M

# Option 2: Anthropic Claude
# AI_PROVIDER=anthropic
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Option 3: OpenAI
# AI_PROVIDER=openai
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4o-mini

# === EMBEDDING MODEL ===
# Option 1: Use local model path (if already downloaded on server)
EMBEDDING_MODEL_PATH=/path/to/your/embedding/model

# Option 2: Download from HuggingFace (will download on first run)
# EMBEDDING_MODEL_PATH=BAAI/bge-m3

# Option 3: Use relative path (if model in backend/models/)
# EMBEDDING_MODEL_PATH=./models/bge-m3

# === DIRECTORIES ===
UPLOAD_DIR=./uploads
OUTPUT_DIR=./outputs
DATA_DIR=./data

# === DATABASE ===
CHROMA_PERSIST_DIR=./data/chroma_db

# === LOGGING ===
LOG_LEVEL=INFO

# === OPTIONAL PERFORMANCE TUNING ===
# CHROMA_BATCH_SIZE=1000
# CUDA_VISIBLE_DEVICES=-1  # Force CPU (set to -1 if no GPU)
